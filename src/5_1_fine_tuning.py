# -*- coding: utf-8 -*-
"""5.1_fine_tuning_v1.ipynb

Automatically generated by Colab.


## **1: Pre-requisitos**

### **1.1. Instalación de Dependencias**
"""

"""### **1.2: Importación de Librerías**"""

import os
import torch
import evaluate
import subprocess
import getpass # Para autenticarse en HF
import pandas as pd
import numpy as np
import datetime
import pytz # Zona horaria
import wandb
import re

from huggingface_hub import HfApi
from transformers import AutoModelForCausalLM, AutoTokenizer #, AutoModelForSeq2SeqLM
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback, GenerationConfig, AutoConfig
from datasets import load_dataset, Dataset
from peft import get_peft_model, LoraConfig, TaskType
from peft import PeftModel, PeftConfig
from sklearn.model_selection import train_test_split
from accelerate import Accelerator
from torch.utils.data import DataLoader
from transformers import BitsAndBytesConfig

"""## **2: Configuración**

### **2.1. Definicion de variables**
"""

def clean_model_name(name):
    """Limpia el nombre del modelo, eliminando caracteres no permitidos para Hugging Face Hub."""
    # Reemplazar espacios y puntos con guiones bajos
    name = name.replace(" ", "_").replace(".", "_")
    # Eliminar cualquier otro carácter especial no permitido
    name = re.sub(r'[^a-zA-Z0-9_-]', '', name)
    return name

BASE_MODEL_NAME = "Qwen/Qwen2.5-3B"  # Asegúrate de seleccionar el modelo adecuado #Qwen/Qwen2.5-1.5B / meta-llama/Llama-3.2-1B / microsoft/phi-1
MODEL_NAME = BASE_MODEL_NAME.split('/')[-1]
CLEAN_MODEL_NAME = clean_model_name(MODEL_NAME)
print(MODEL_NAME)
print(CLEAN_MODEL_NAME)

FILE_PATH = '/data/dataset20.tsv'
FILE_NAME = os.path.splitext(os.path.basename(FILE_PATH))[0]
print(FILE_NAME)

"""### **2.2. Login Hugging Face**"""

def set_huggingface_hub_token():
    """Configura el token para Hugging Face Hub solicitándolo al usuario y verificando su existencia."""
    if "HUGGING_FACE_HUB_TOKEN" in os.environ:
        print("Token ya configurado.")
    else:
        token = getpass.getpass("Token: ") # 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
        if token:  # Verificar que el token no esté vacío
            os.environ["HUGGING_FACE_HUB_TOKEN"] = token
            print("Token configurado exitosamente.")
        else:
            raise ValueError("No se ingresó un token válido.")

# Uso de la función
try:
    set_huggingface_hub_token()
except ValueError as e:
    print(e)


"""### **2.3. Configuración del Modelo**"""

def get_device():
    """Configura y retorna el dispositivo basado en la disponibilidad de CUDA."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Usando dispositivo: {device}")
    return device

def set_device():
    """Configura y retorna el dispositivo para entrenamiento basado en la disponibilidad de CUDA."""
    if torch.cuda.is_available():
        print("CUDA está disponible. Usando GPU.")
        return torch.device("cuda")
    else:
        print("CUDA no está disponible. Usando CPU.")
        return torch.device("cpu")

def load_model_and_tokenizer(base_model_name):
    """Carga un modelo y un tokenizador, manejando posibles excepciones durante la carga."""
    try:
        model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map="auto")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True, clean_up_tokenization_spaces=True)
        print("Modelo y tokenizador cargados exitosamente.")
        return model, tokenizer
    except Exception as e:
        print(f"Error al cargar el modelo o el tokenizador: {e}")
        return None, None

# Uso de las funciones
device = set_device()
model_base, tokenizer = load_model_and_tokenizer(BASE_MODEL_NAME)

def load_model_optimized(base_model_name, use_4bit=False):
    try:
        if use_4bit:
            # Código 1 con mejoras
            compute_dtype = torch.float16
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True  # Asegúrate de que esta opción esté correctamente definida
            )
        else:
            # Código 2 con mejoras
            compute_dtype = torch.bfloat16
            quantization_config = None  # Para evitar que se pase un objeto no necesario

        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=compute_dtype,
            offload_folder="./offload"
            # low_cpu_mem_usage=True
        )

        # Optimizaciones comunes
        #model.gradient_checkpointing_enable()
        #model.enable_input_require_grads()

        tokenizer = AutoTokenizer.from_pretrained(base_model_name)

        return model, tokenizer

    except Exception as e:
        print(f"Error: {e}")
        return None, None

if torch.cuda.is_available():
    num_gpus = torch.cuda.device_count()
    print(f"Número de GPUs disponibles: {num_gpus}")
    for i in range(num_gpus):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
else:
    print("No hay GPUs disponibles.")

num_cores = os.cpu_count()
print(f"Número de núcleos de CPU: {num_cores}")


"""## **3: Carga y Preparación del Dataset**"""

df = pd.read_csv(FILE_PATH, sep='\t')
df.head()

"""### **3.1. Limpiar Datos**"""

df['description'] = df['description'].str.replace(r'\\+n', '\n', regex=True)

# Función optimizada con prints para depuración
def clean_description(text):
    """Limpia la columna 'description' y muestra el progreso."""

    #print(f"\nTexto original:\n{text}")

    # 1. Reemplaza caracteres especiales al inicio por '/*'
    text = re.sub(r"^[^\w\s]+", "/*", text)

    # 2. Reemplaza secuencia específica al inicio por '/*\n'
    text = re.sub(r"^/\*\n\*\n", "/*\n", text)

    # 3. Reemplaza caracteres especiales al final por '*/'
    text = re.sub(r"[^\w\s]+$", "*/", text)

    # 4. Elimina la secuencia "', */" al final de la cadena
    text = re.sub(r"', \*/$", "", text)

    # 5. Remueve caracteres especiales excepto los permitidos
    text = re.sub(r"[^\w\s\/\*;@<>{}]", "", text)

    # 6. Reemplaza múltiples patrones con slashes y comas por vacío
    text = re.sub(r"(/\s*,\s*//|\s*,\s*,\s*//|,\s*,\s*,\s*,\s*//)", "", text)

    # 7. Remover etiquetas <p> y </p>
    text = re.sub(r'<p>|<\/p>', '', text)  # Remueve <p> y </p>
    #print(f"\nDespués de eliminar etiquetas <p> y </p>:\n{text}")

    return text

# Aplica la función a la columna 'description' para todos los registros
df['description'] = df['description'].apply(clean_description)

"""### **3.2. Quitar Registros**"""

# Calculating the length of each cell in each column
df['num_characters_description'] = df['description'].apply(lambda x: len(x))
df['num_characters_focal'] = df['focal_method'].apply(lambda x: len(x))
df['num_characters_test'] = df['test_case'].apply(lambda x: len(x))

# Eliminar registros con num_characters_description menores a 50
df = df[df['num_characters_description'] > 50]
print(df.shape)

# Eliminar registros con num_characters_test menores a 120
df = df[df['num_characters_test'] > 120]
print(df.shape)

# Eliminar registros con num_characters_test mayor a 3000
df = df[df['num_characters_test'] <= 2000]
print(df.shape)

df = df[['description', 'focal_method', 'test_case']]
print(df.shape)

"""### **3.3. Convertir a dataset**"""

dataset = Dataset.from_pandas(df)
#dataset = Dataset.from_pandas(df['description', 'focal_method', 'test_case'])
print(dataset)
print("Dataset cargado")

"""## **4: Entrenamiento**

### **4.1. Tokenizacion**
"""
def tokenize_preprocess(examples, tokenizer, max_length=256):
    """Tokeniza los inputs y los targets, y prepara las entradas y etiquetas para el modelo."""
    model_inputs = tokenizer(examples['description'], truncation=True, padding='max_length', max_length=max_length)

    # Tokenizar los targets utilizando el tokenizer como target_tokenizer
    labels = tokenizer(examples['test_case'], truncation=True, padding='max_length', max_length=max_length)
    model_inputs['labels'] = labels['input_ids']

    return model_inputs

# Función para preparar los conjuntos de datos
def prepare_datasets(dataset: Dataset, tokenizer, test_size: float=0.2, validation_size: float=0.5):
    """Aplica la tokenización y divide el dataset en entrenamiento, prueba y validación."""
    # Configurar el token de padding como el token de fin de secuencia
    tokenizer.pad_token = tokenizer.eos_token

    # Aplicar preprocesamiento al dataset
    tokenized_dataset = dataset.map(lambda x: tokenize_preprocess(x, tokenizer), batched=True)

    # Dividir el dataset en 80% train y 20% resto
    train_test_split = tokenized_dataset.train_test_split(test_size=test_size)
    test_validation_split = train_test_split['test'].train_test_split(test_size=validation_size)

    return {
        'train': train_test_split['train'],
        'test': test_validation_split['test'],
        'validation': test_validation_split['train']
    }

# Uso de las funciones
tokenized_datasets = prepare_datasets(dataset, tokenizer)
print(tokenized_datasets)

"""### **4.2. Configuración LoRA**"""

def configure_lora(task_type, r, lora_alpha, lora_dropout):
    """Configura y retorna los ajustes de LoRA."""
    lora_config = LoraConfig(
        task_type=task_type,
        inference_mode=False,  # Modo de entrenamiento
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout
    )
    return lora_config

def apply_lora_to_model(model_base, lora_config):
    """Aplica LoRA al modelo y lo mueve al dispositivo especificado."""
    try:
        model = get_peft_model(model_base, lora_config)  # Asumiendo que esta función ya está definida
        #model = model.to(device)
        print("Modelo adaptado con LoRA y movido al dispositivo correctamente.")
        return model
    except Exception as e:
        print(f"Error al aplicar LoRA o mover el modelo al dispositivo: {e}")
        return None

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param} %"
    )

"""### **4.3. Configuración Finetuning PEFT**"""

class MetricsLogger(TrainerCallback):
    """Callback to log metrics to WandB and prevent duplicate entries."""
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            # Define qué métricas quieres loguear en WandB
            metrics_to_log = ['loss', 'eval_loss', 'eval_accuracy', 'learning_rate', 'step', 'epoch']
            # Filtra y loguea solo las métricas definidas
            wandb.log({k: v for k, v in logs.items() if k in metrics_to_log}, step=state.global_step)

# Función para calcular métricas
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {"accuracy": (predictions == labels).mean()}

# Función para guardar y registrar el modelo
def save_and_log_model(trainer, model_name, output_dir, current_datetime):
    try:
        # Guarda únicamente los pesos del modelo en el directorio especificado para su fácil recarga o distribución.
        trainer.model.save_pretrained(output_dir)

        # Guarda el modelo completo, incluyendo pesos, configuración, y opcionalmente el estado del optimizador y scheduler, ideal para reanudar el entrenamiento.
        trainer.save_model(output_dir) #

        # Limpiar el nombre del modelo para usarlo en el nombre del artefacto
        sanitized_model_name = re.sub(r"[^a-zA-Z0-9_.-]", "_", model_name)
        artifact_name = f"peft_model_{sanitized_model_name}_{current_datetime}"

        # Crear y registrar el artefacto
        model_artifact = wandb.Artifact(name=artifact_name, type="model", description=f"Fine-tuned PEFT model based on {model_name}")
        model_artifact.add_dir(output_dir)
        wandb.log_artifact(model_artifact)

    except Exception as e:
        print(f"Error during model saving or artifact logging: {e}")

def init_wandb(training_args, project_name, model_name, file_name, current_datetime):
    wandb.init(
        project=project_name,
        name=f"Experimento_{model_name}_{file_name}_{current_datetime}",
        config=vars(training_args)
    )

def train_model(model, training_args, train_dataset, eval_dataset, compute_metrics, MetricsLogger, model_name, output_dir, current_datetime):
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[MetricsLogger()]
    )
    try:
        results = trainer.train()
        save_and_log_model(trainer, model_name, output_dir, current_datetime)
    except Exception as e:
        print(f"Ha ocurrido un error durante el entrenamiento: {e}")
    #finally:
    #    wandb.finish()

def get_device():
    if torch.cuda.is_available():
        try:
            # Probar si hay suficiente memoria en la GPU
            torch.cuda.empty_cache()
            return torch.device('cuda')
        except RuntimeError:
            print("Memoria de GPU insuficiente, cambiando a CPU.")
            return torch.device('cpu')
    else:
        return torch.device('cpu')

device = get_device()
print(device)

#model = model.to(device)
def configure_training():
    lima_tz = pytz.timezone('America/Lima')
    current_datetime = datetime.datetime.now(lima_tz).strftime("%Y%m%d_%H%M%S")
    output_dir = f"./results/run_{current_datetime}"
    print(output_dir)
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=8,
        learning_rate=5e-4,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        eval_strategy="steps",
        #save_strategy="epoch",
        eval_steps=1,
        save_steps=1,
        logging_dir='./logs',
        logging_steps=2,
        fp16=True,
        save_total_limit=1,
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False,
        #gradient_checkpointing=True,
        #optim="paged_adamw_8bit",
        dataloader_num_workers=2
        #offload_folder="./offload",       # Carpeta para offloading de memoria
        #device_map="auto"                 # Uso automático de la GPU
        #optim="adamw_torch_fused"
    )
    return training_args, output_dir, current_datetime

"""### **4.4. Ejecucion**"""

PROJECT_NAME_WANDB="FinetuningTestCases_khipu"
LORA_RANK = 8  # Número de matrices de bajo rango (ajusta según tu GPU)
LORA_ALPHA = 16  # Factor de escala para LoRA
LORA_DROPOUT = 0.05  # Dropout para regularización
TASK_TYPE = TaskType.CAUSAL_LM
#DEVICE = get_device() # o 'cpu' dependiendo de tu configuración

lora_config = configure_lora(task_type=TASK_TYPE, r=LORA_RANK, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT) # Configurar LoRA
peft_model_gemma = apply_lora_to_model(model_base, lora_config) # Aplicar configuración de LoRA al modelo y moverlo al dispositivo
print_trainable_parameters(peft_model_gemma) # Imprimir parámetros entrenables

# START TRAIN
if __name__ == "__main__":
    peft_training_args, output_dir, current_datetime = configure_training()
    init_wandb(peft_training_args, PROJECT_NAME_WANDB, MODEL_NAME, FILE_NAME, current_datetime) #cf350a1688d2b22ef3f22bee644b51b16b61946e
    train_model(peft_model_gemma, peft_training_args, tokenized_datasets["train"],  tokenized_datasets["validation"], compute_metrics, MetricsLogger, MODEL_NAME, output_dir, current_datetime)

#wandb.finish()

"""### **5 Evaluación**"""

rouge = evaluate.load('rouge')
bleu = evaluate.load("bleu")

"""### **5.1. Evaluando con Rouge**"""

# Verificamos si CUDA (GPU) está disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

descriptions = tokenized_datasets['test'][0:2]['description']
human_baseline_test_case = tokenized_datasets['test'][0:2]['test_case']

base_model_test_cases = []
peft_model_test_cases = []

for _, description in enumerate(descriptions):
    prompt = (
        f"Generate a test JUnit for the following method description: "
        f"{description}\n\n"
        f"Test JUnit in Java:\n"
    )

    # Tokenizamos y creamos la attention_mask, moviendo los tensores al dispositivo adecuado (CPU o GPU)
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs.input_ids.to(device)
    attention_mask = inputs.attention_mask.to(device)

    # Movemos el modelo base al mismo dispositivo que los tensores
    #model_base.to(device)
    base_model_outputs = model_base.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,  # Se pasa la atención aquí
        max_new_tokens=200,
        pad_token_id=tokenizer.eos_token_id  # Usamos eos_token_id como pad_token_id
    )
    base_model_text_output = tokenizer.decode(base_model_outputs[0], skip_special_tokens=True)
    base_model_test_cases.append(base_model_text_output)

    # Movemos también el peft_model al mismo dispositivo
    peft_model_gemma.to(device)
    peft_model_outputs = peft_model_gemma.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,  # Se pasa la atención aquí
        max_new_tokens=200,
        pad_token_id=tokenizer.eos_token_id  # Usamos eos_token_id como pad_token_id
    )
    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)
    peft_model_test_cases.append(peft_model_text_output)

# Verificar que las listas tengan la misma longitud
if len(human_baseline_test_case) == len(base_model_test_cases) == len(peft_model_test_cases):
    zipped_summaries = list(zip(human_baseline_test_case, base_model_test_cases, peft_model_test_cases))

    df_report = pd.DataFrame(zipped_summaries, columns = ['HUMAN_BASELINE_TEST_CASES', 'BASE_MODEL_TEST_CASES', 'PEFT_'+MODEL_NAME.upper()+'_MODEL_TEST_CASES'])
    df_report
else:
    raise ValueError("Las listas de casos de prueba no tienen la misma longitud.")


base_model_results_rouge = rouge.compute(
    predictions=base_model_test_cases,
    references=human_baseline_test_case[0:len(base_model_test_cases)],
    use_aggregator=True,
    use_stemmer=True,
)

peft_model_results_rouge = rouge.compute(
    predictions=peft_model_test_cases,
    references=human_baseline_test_case[0:len(peft_model_test_cases)],
    use_aggregator=True,
    use_stemmer=True,
)

print('ORIGINAL MODEL:')
print(base_model_results_rouge)

print('\nPEFT MODEL:')
print(peft_model_results_rouge)

"""### **5.2. Evaluando con Bleu**"""

# Evaluar el modelo base
base_model_results_bleu = bleu.compute(
    predictions=base_model_test_cases,
    references=[[ref] for ref in human_baseline_test_case[0:len(base_model_test_cases)]]
)

# Evaluar el modelo PEFT
peft_model_results_bleu = bleu.compute(
    predictions=peft_model_test_cases,
    references=[[ref] for ref in human_baseline_test_case[0:len(peft_model_test_cases)]]
)

# Imprimir resultados
print('BASE MODEL BLEU SCORE:')
print(base_model_results_bleu)

print('\nPEFT MODEL BLEU SCORE:')
print(peft_model_results_bleu)

# Registrar los resultados de ROUGE en la sección "ROUGE"
wandb.log({
    "Test/Base_Model": base_model_results_rouge,
    "Test/PEFT_Model": peft_model_results_rouge
})

# Registrar los resultados de BLEU en la sección "BLEU"
wandb.log({
    "Test/Base_Model_BLEU": base_model_results_bleu['bleu'],  # Accediendo al score directamente
    "Test/PEFT_Model_BLEU": peft_model_results_bleu['bleu']
})

# Finalizar sesión de WandB
wandb.finish()

"""## **Guardar y cargar modelo en HF**

### Guardar Modelo
"""

# Inicializar API
api = HfApi()

def save_model_locally(model, model_path):
    """Guarda el modelo localmente."""
    model.save_pretrained(model_path)
    tokenizer.save_pretrained(model_path)
    print(f"Modelo guardado localmente en {model_path}.")

def create_or_get_repo(repo_id, token, private=False):
    """Crea un repositorio en Hugging Face Hub si no existe."""
    try:
        api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True)
        print(f"Repositorio listo: {repo_id}")
    except Exception as e:
        print(f"Error al crear o acceder al repositorio: {e}")

def upload_lora_adapters(repo_id, lora_model_path, token):
    """Sube los adaptadores LoRA al Hugging Face Hub."""
    api.upload_folder(repo_id=repo_id, folder_path=lora_model_path, token=token)
    print(f"Adaptadores LoRA subidos exitosamente en {repo_id}.")

def save_and_upload_quantized_model(model, model_name, tokenizer, hf_user_name, token, quantization_method="q4_k_m"):
    """Guarda el modelo en formato GGUF cuantizado y lo sube al Hugging Face Hub."""
    model_gguf_path = f"{model_name}-gguf"
    model.save_pretrained_gguf(model_gguf_path, tokenizer, quantization_method=quantization_method)
    model.push_to_hub_gguf(
        repo_id=f"{hf_user_name}/{model_gguf_path}",
        tokenizer=tokenizer,
        quantization_method=quantization_method,
        token=token
    )
    print(f"Modelo GGUF cuantizado subido en {hf_user_name}/{model_gguf_path}.")

# Configuraciones generales
HF_USERNAME = "Judiht"
HF_TOKEN = os.getenv("HUGGING_FACE_HUB_TOKEN")  # Accede al token de Hugging Face desde una variable de entorno
#FILE_NAME2 = "testcase_generator_16df"
REPO_LOCAL_NAME = f"{CLEAN_MODEL_NAME}_{FILE_NAME}_{current_datetime}"
REPO_HF_NAME = f"{HF_USERNAME}/{REPO_LOCAL_NAME}"

# Verificación del token
if HF_TOKEN is None:
    raise ValueError("El token de Hugging Face no está configurado. Asegúrate de definir HUGGING_FACE_HUB_TOKEN en las variables de entorno.")

# Flujo principal
# Guardar el modelo base localmente
save_model_locally(peft_model_gemma, REPO_LOCAL_NAME)

# Crear el repositorio en Hugging Face Hub
create_or_get_repo(REPO_HF_NAME, HF_TOKEN)

# Subir los adaptadores LoRA
upload_lora_adapters(REPO_HF_NAME, REPO_LOCAL_NAME, HF_TOKEN)